{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with Pytorch for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HWv6NA5YKpLU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I  downloaded a small dataset containing images of bees and ants from Kaggle and saved it in the local directory. \n",
    "The dataset is too small to train a deep neural network model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mtrain\u001b[m\u001b[m/ \u001b[34mval\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls data/hymenoptera_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     124\n"
     ]
    }
   ],
   "source": [
    "#There are 124 images of ants for training and 70 images for validation.\n",
    "\n",
    "!ls data/hymenoptera_data/train/ants/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      70\n"
     ]
    }
   ],
   "source": [
    "!ls data/hymenoptera_data/val/ants/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     121\n"
     ]
    }
   ],
   "source": [
    "# There are 121 images of bees for training and 83 images for validation.\n",
    "                                                \n",
    "!ls data/hymenoptera_data/train/bees/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      83\n"
     ]
    }
   ],
   "source": [
    "ls data/hymenoptera_data/val/bees/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbzMwCbOLF4T"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The data must be preprocessed before subplying to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rgMYPb-DLP3i"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456,0.406], \n",
    "        [0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406], \n",
    "        [0.229, 0.224, 0.225])])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Applying the transformations to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FsfLVtWOl-8u"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "          root='data/hymenoptera_data/train',\n",
    "          transform=train_transforms)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "            root='data/hymenoptera_data/val',\n",
    "            transform=val_transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We need dataloaders to form batches of data to feed them into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yNCNuggQl_IO"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=4,\n",
    "            shuffle=True, \n",
    "            num_workers=4)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=4,\n",
    "            shuffle=True, \n",
    "            num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4XwasvqWcBp"
   },
   "source": [
    "# Model Design"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will use a pretrained ResNet18 model from torchvision module. ResNet18 was trained on ImageNet dataset that contains millions\n",
    "of images distributed among thousands of categories. In this project, we need to classify images into two categories, bees and ants,\n",
    "so we will modify the output layer of ResNet18 to output two values. Then we will train the modified model on the bees and ants\n",
    "dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "e656dfaaa299499bbaa6443515a5fc82",
      "06480ae1122a4baab79943db4b914509",
      "3fdaea55867247278f48db9368e11cee",
      "d686ae919ceb42f3a107b4c0edda3ed1",
      "8d0244f593d8422aa185c297ae06dccb",
      "64071b2ff2fa4673a69fb2975e828b89",
      "8bc5f59d7a954762aa9377837aebf3aa",
      "4e1154d9ad304fa7a4e1890520384162"
     ]
    },
    "executionInfo": {
     "elapsed": 6454,
     "status": "ok",
     "timestamp": 1614906727387,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 300
    },
    "id": "Kbf-3OlcWV2s",
    "outputId": "861454d7-64f8-49f9-aa42-688f55ec20c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=1000, bias=True)\n",
      "Linear(in_features=512, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import ResNet18_Weights\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "print(model.fc)\n",
    "# out: Linear(in_features=512, out_features=1000, bias=True)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "print(model.fc)\n",
    "# out: Linear(in_features=512, out_features=2, bias=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ResNet18 contained 1000 units in the output layer, we modified the output layer to output two values.\n",
    "We will retrain the whole model not only the modified layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.bn1.weight\n",
      "layer1.1.bn1.bias\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.bn2.weight\n",
      "layer1.1.bn2.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.0.downsample.1.weight\n",
      "layer2.0.downsample.1.bias\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.bn1.weight\n",
      "layer2.1.bn1.bias\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.bn2.weight\n",
      "layer2.1.bn2.bias\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.bn1.weight\n",
      "layer3.0.bn1.bias\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.bn2.weight\n",
      "layer3.0.bn2.bias\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.0.downsample.1.weight\n",
      "layer3.0.downsample.1.bias\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.bn1.weight\n",
      "layer3.1.bn1.bias\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.bn2.weight\n",
      "layer3.1.bn2.bias\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.bn1.weight\n",
      "layer4.0.bn1.bias\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.bn2.weight\n",
      "layer4.0.bn2.bias\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.0.downsample.1.weight\n",
      "layer4.0.downsample.1.bias\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.bn1.weight\n",
      "layer4.1.bn1.bias\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.bn2.weight\n",
      "layer4.1.bn2.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laTFYetEWsC1"
   },
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "z7ViIt5dRo1D"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) \n",
    "exp_lr_scheduler = StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199646,
     "status": "ok",
     "timestamp": 1614906920597,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 300
    },
    "id": "qiJB0LQzB0ES",
    "outputId": "92e7b4d0-d73f-4c04-de50-0a1b327f62b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss = 0.2001 Acc = 0.6844; Val: Loss = 0.1505 Acc = 0.7756\n",
      "Train: Loss = 0.1560 Acc = 0.7295; Val: Loss = 0.0652 Acc = 0.9167\n",
      "Train: Loss = 0.1105 Acc = 0.8197; Val: Loss = 0.0657 Acc = 0.9167\n",
      "Train: Loss = 0.1354 Acc = 0.7787; Val: Loss = 0.0766 Acc = 0.9167\n",
      "Train: Loss = 0.1937 Acc = 0.7623; Val: Loss = 0.0827 Acc = 0.8974\n",
      "Train: Loss = 0.1522 Acc = 0.7664; Val: Loss = 0.1850 Acc = 0.7244\n",
      "Train: Loss = 0.1592 Acc = 0.7828; Val: Loss = 0.1779 Acc = 0.8013\n",
      "Train: Loss = 0.1093 Acc = 0.8484; Val: Loss = 0.0678 Acc = 0.9038\n",
      "Train: Loss = 0.0901 Acc = 0.8648; Val: Loss = 0.0606 Acc = 0.9167\n",
      "Train: Loss = 0.0763 Acc = 0.8689; Val: Loss = 0.0575 Acc = 0.9167\n",
      "Train: Loss = 0.0695 Acc = 0.8730; Val: Loss = 0.0558 Acc = 0.9423\n",
      "Train: Loss = 0.0911 Acc = 0.8279; Val: Loss = 0.0553 Acc = 0.9295\n",
      "Train: Loss = 0.0754 Acc = 0.8689; Val: Loss = 0.0523 Acc = 0.9231\n",
      "Train: Loss = 0.0793 Acc = 0.8648; Val: Loss = 0.0584 Acc = 0.9295\n",
      "Train: Loss = 0.0656 Acc = 0.8811; Val: Loss = 0.0511 Acc = 0.9295\n",
      "Train: Loss = 0.0836 Acc = 0.8197; Val: Loss = 0.0447 Acc = 0.9423\n",
      "Train: Loss = 0.0784 Acc = 0.8730; Val: Loss = 0.0454 Acc = 0.9359\n",
      "Train: Loss = 0.0595 Acc = 0.8852; Val: Loss = 0.0520 Acc = 0.9231\n",
      "Train: Loss = 0.0738 Acc = 0.8811; Val: Loss = 0.0472 Acc = 0.9295\n",
      "Train: Loss = 0.0604 Acc = 0.8893; Val: Loss = 0.0591 Acc = 0.9295\n",
      "Train: Loss = 0.0900 Acc = 0.8525; Val: Loss = 0.0494 Acc = 0.9359\n",
      "Train: Loss = 0.0668 Acc = 0.8730; Val: Loss = 0.0477 Acc = 0.9295\n",
      "Train: Loss = 0.0797 Acc = 0.8770; Val: Loss = 0.0441 Acc = 0.9359\n",
      "Train: Loss = 0.0831 Acc = 0.8689; Val: Loss = 0.0515 Acc = 0.9423\n",
      "Train: Loss = 0.0762 Acc = 0.8648; Val: Loss = 0.0456 Acc = 0.9423\n",
      "Train: Loss = 0.0610 Acc = 0.9180; Val: Loss = 0.0520 Acc = 0.9423\n",
      "Train: Loss = 0.0710 Acc = 0.8770; Val: Loss = 0.0539 Acc = 0.9295\n",
      "Train: Loss = 0.0638 Acc = 0.8730; Val: Loss = 0.0596 Acc = 0.9167\n",
      "Train: Loss = 0.0491 Acc = 0.9016; Val: Loss = 0.0431 Acc = 0.9359\n",
      "Train: Loss = 0.0629 Acc = 0.8852; Val: Loss = 0.0425 Acc = 0.9423\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training phase\n",
    "    \n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()/inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels)/inputs.size(0)\n",
    "\n",
    "  \n",
    "    exp_lr_scheduler.step() \n",
    "    train_loss = running_loss/len(train_loader)\n",
    "    train_acc = running_corrects/len(train_loader)\n",
    "\n",
    "    \n",
    "    # Validation phase\n",
    "    \n",
    "    model.eval() \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() / inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels)/inputs.size(0)\n",
    "\n",
    "    val_loss = running_loss/len(val_loader)\n",
    "    val_acc = running_corrects.double()/len(val_loader)\n",
    "    \n",
    "    print(\"Train: Loss = {:.4f} Acc = {:.4f}; Val: Loss = {:.4f} Acc = {:.4f}\".format(train_loss, train_acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dehbYvUZXROg"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./modified_resnet18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1fjXxFO-SfS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO0sdrjU+nMP7dZ5HcwbL02",
   "collapsed_sections": [],
   "name": "04_01_Transfer_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06480ae1122a4baab79943db4b914509": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fdaea55867247278f48db9368e11cee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64071b2ff2fa4673a69fb2975e828b89",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d0244f593d8422aa185c297ae06dccb",
      "value": 46827520
     }
    },
    "4e1154d9ad304fa7a4e1890520384162": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64071b2ff2fa4673a69fb2975e828b89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bc5f59d7a954762aa9377837aebf3aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d0244f593d8422aa185c297ae06dccb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d686ae919ceb42f3a107b4c0edda3ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e1154d9ad304fa7a4e1890520384162",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8bc5f59d7a954762aa9377837aebf3aa",
      "value": " 44.7M/44.7M [03:14&lt;00:00, 240kB/s]"
     }
    },
    "e656dfaaa299499bbaa6443515a5fc82": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fdaea55867247278f48db9368e11cee",
       "IPY_MODEL_d686ae919ceb42f3a107b4c0edda3ed1"
      ],
      "layout": "IPY_MODEL_06480ae1122a4baab79943db4b914509"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
